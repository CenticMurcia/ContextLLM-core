{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.vectorstores import VectorStore \n",
    "from langchain_core.documents.base import Document\n",
    "from langchain.load import dumps, loads\n",
    "\n",
    "\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from typing import List\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joseacentic/centic/LLM/envPRUEBA/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(base_url=\"http://localhost:11434\",\n",
    "             model=\"mistral\",\n",
    "             verbose=True,  \n",
    "             callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))\n",
    "\n",
    "hugg_embeddings = HuggingFaceEmbeddings(model_name= \"mxbai-embed-large-v1\")\n",
    "\n",
    "hugg_emb_bgem3 = HuggingFaceEmbeddings(model_name = \"BAAI/bge-m3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "myFile = 'annualreport.pdf'\n",
    "loader = PyPDFLoader(\"DATA/\" + myFile)\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len\n",
    "    )\n",
    "\n",
    "all_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(\n",
    "        documents=all_splits,\n",
    "        embedding=hugg_embeddings\n",
    "    )\n",
    "\n",
    "vectorstore_bgem3 = Chroma.from_documents(\n",
    "        documents=all_splits,\n",
    "        embedding=hugg_emb_bgem3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Explicación del retriever, en particular la función _get_relevant_documents() para saber como usar el retriever cuando se quieran añadir más \n",
    "condiciones en el rrf: \n",
    "Esta clase está estructurada para que con las funciones rrf() y get_fused_scores() se puedan ir añadiendo tantos metodos de recuperación de \n",
    "contexto como se quiera. (Siempre que los documentos recuperados tengan un orden concreto, para los que no, ya está el método fused_scores_literal()\n",
    "que asigna a todos los documentos el mismo valor). \n",
    "¿Cómo funciona y se puede automatizar la función _get_relevant_documents()?\n",
    "Supongamos que tenemos n criterios de recuperación de contextos (en este ejemplo tenemos 4, literal, mxbai l2, mxbai cos, bge cos)\n",
    "En primer lugar, se necestita ir calculando los scores de los documentos que vamos obteniendo para ello, se va haciendo uso de la función get_fused_scores()\n",
    "scores1 =  get_fused_scores({},lista1), scores2 = get_fused_scores(scores1,lista2), ... , scoresn = get_fused_scores(scores(n-1), listan)\n",
    "Esto lo que va haciendo es ir acumulando los scores de acuerdo al valor que se ha decidido dar. En este caso, 1/1+rank. \n",
    "Por ultimo una vez tengamos todos los documentos con sus scores se hace uso de la función rrf con los ultimos scores, que se encarga de dar la lista final de \n",
    "documentos ordenada sin los scores ya para que la procese la función del retriever. \n",
    "rrf(scores(n)) puesto que esta función desde dentro ya se encarga de calcular los scores para la ultima lista.\n",
    "\n",
    "'''\n",
    "class CustomRetriever_advanced(BaseRetriever):\n",
    "    '''En este caso se necesitan dos db vectoriales que almacenen los embeddings que genera cada modelo diferente. Se pueden usar\n",
    "    tantos modelos de generación de embeddings como se quiera simplemente habrá que pasar en esta clase del retriever personalizado \n",
    "    las bases de datos correspondientes. También se va a añadir una variable k que será el número de documentos que se quieran recuperar\n",
    "    Por ejemplo si k = 10 pues el retriever generará un contexto con los 10 documentos que mayor score hayan generado. En el caso anterior \n",
    "    que k no se especificaba, en los documentos se metían todos los que se iban recuperando de cada método. '''\n",
    "    vs: VectorStore\n",
    "    vs_2: VectorStore\n",
    "    k: int\n",
    "    '''fused_scores es un diccionario que va a ir actualizando la solucion de rrf por lo que en la primera\n",
    "    iteración fused_scores = {} (diccionario vacio) mientras que para las demás iteraciones será la solución\n",
    "    de esta función junto con las diferentes listas que queramos ir pasandole el rrf'''\n",
    "    \n",
    "    '''\n",
    "    Cada vez que se hace una llamada a la función rrf nos da una lista de objetos Document (sin el score) que están ya listos \n",
    "    para pasar al retrievalQA. Es necesario pasarle la lista de scores que se va obteniendo conforme se van usando técnicas en\n",
    "    el reranking. \n",
    "    '''\n",
    "    def rrf(self,fused_scores):\n",
    "        reranked_results = {doc: score for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)}\n",
    "        #falta pasar estos resultados a una lista de documentos que es lo que devuelve el retriver (loads)\n",
    "        lista_rerank = []\n",
    "        print('Documentos finales (score)')\n",
    "        for doc, score in reranked_results.items():\n",
    "            lista_rerank.append(loads(doc))\n",
    "            print('------')\n",
    "            print(score)\n",
    "        return lista_rerank\n",
    "    \n",
    "    '''\n",
    "    La función get_fused_scores, nos sirve para ir actualizando los scores de una lista de documetnos dada. Por ejemplo, si estamos \n",
    "    en la primera iteración del rrf, previus_fused = {} y esta función devolverá un diccionario actualizado con los documetnos de la lista \n",
    "    y su correspondiente score. Este diccionario será necesario para pasarselo luego en la función rrf. \n",
    "    Cuando no estamos en la primera iteración y previus_fused ya no es un diccionario vacio, se le pasará un diccionario de documentos y \n",
    "    scores junto con una nueva lista que tendrá los documentos que queremos incorporar a la lista de scores. \n",
    "    '''   \n",
    "    def get_fused_scores(self,previus_fused, lista):\n",
    "        for rank, doc in enumerate(lista):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in previus_fused:\n",
    "                previus_fused[doc_str] = 0\n",
    "            previus_score = previus_fused[doc_str]\n",
    "            print(f'scores previos {previus_score}')\n",
    "            previus_fused[doc_str] += 1 / (rank + 1)\n",
    "        return previus_fused\n",
    "    \n",
    "    #Funciona\n",
    "    '''\n",
    "    Nota: Esta función no haría falta si no se le diera la misma puntuación a cada documento obtenido en la búsqueda literal. Esto se debe a que \n",
    "    cuando se buscan documentos de forma literal todos han de tener el mismo valor. No se puede distinguir de esta manera si hay uno \"mejor\" que otro. \n",
    "    '''\n",
    "    def fused_scores_literal(self, lista):\n",
    "        fused_scores = {}\n",
    "        for doc in lista:\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            previus_score = fused_scores[doc_str]\n",
    "            print(f'scores previos {previus_score}')\n",
    "            fused_scores[doc_str] = 0.5\n",
    "        return fused_scores\n",
    "    ''' \n",
    "    Genera una consulta para que sea dinámica la busqueda literal en la base de datos, dada una lista de palabras clave. \n",
    "    El método de busqueda literal en la base de datos chroma, necesita el siguiente formato\n",
    "    {\n",
    "    \"$and\": [\n",
    "        {\"$contains\": \"key_word1\"},\n",
    "        {\"$contains\": \"key_word2\"}\n",
    "        ]\n",
    "    }\n",
    "    Con lo cual, este método en función de las palabras clave que tenga nuestra query, se van añadiendo a la consulta y no se\n",
    "    necesita ir creando una consulta a mano para cada búsqueda literal que tenga diferente numero de palabras clave. \n",
    "    '''\n",
    "    def generaConsulta(self,key_words):\n",
    "        busquedas = []\n",
    "        for w in key_words:\n",
    "            conta = {\"$contains\":str(w)}\n",
    "            busquedas.append(conta)\n",
    "        #Duda: Poner un or en vez de un and. En el caso de que existan muchos keywords puede ser dificil encontrar documentos que los contengan todas. \n",
    "        consulta = {\"$and\": busquedas}\n",
    "        return consulta\n",
    "    \n",
    "    '''Corrective RAG:\n",
    "    La función CRAG, recibe una query (pregunta) y una serie de documentos que han sido recuperados con el objetivo de responder la pregunta\n",
    "    de forma correcta. En este caso, se crea un LLM que sea capaz de evaluar cada uno de estos documentos y decida si su contenido es adecuado\n",
    "    para responder la pregunta. Los documentos que se consideren adecuados son aquellos que se devuelven en esta función. \n",
    "    '''\n",
    "    def CRAG(self, query, docs):\n",
    "\n",
    "        prompt = PromptTemplate(\n",
    "            template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "            Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "            Here is the user question: {question} \\n\n",
    "            If the document contains keywords related to the user question, grade it as relevant. \\n\n",
    "            It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "            Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "            Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\"\"\",\n",
    "            input_variables=[\"question\", \"document\"],\n",
    "        )\n",
    "\n",
    "        retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "\n",
    "        valid_docs = []\n",
    "\n",
    "        for d in docs:\n",
    "            score = retrieval_grader.invoke({\"question\": query, \"document\": d.page_content})\n",
    "            if score['score'] == \"yes\":\n",
    "                valid_docs.append(d)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        return valid_docs\n",
    "    \n",
    "    '''\n",
    "    En el RetrievalQA, que se pasa en la cadena, el retriever que se usa por la clase para obtener los diferentes documentos hace uso de una función\n",
    "    llamada get_relevant_documents() que viene predeterminada en el retriever base. Cuando hacemos un retriever personalizado, es necesario crear una\n",
    "    función _get_relevant_documents(), con _ delante del mismo nombre anterior para que a la hora de recuperar documentos la función RetrievalQA haga \n",
    "    uso de la función que hemos personalizado para recuperar los documentos. Solo tenemos que tener en cuenta que al aplicar los cambios que queremos\n",
    "    al personalizar esta función, se devuelva una lista de documentos (objetos Document)\n",
    "    '''\n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
    "        ) -> List[Document]:\n",
    "        #documentos\n",
    "        spacyModel = spacy.load(\"en_core_web_sm\")\n",
    "        list = self.vs.get(\n",
    "            where_document=self.generaConsulta(spacyModel(query).ents)\n",
    "            )\n",
    "        \n",
    "        literal_docs = []\n",
    "        for i in range(len(list['ids'])):\n",
    "            doc = Document(page_content=list['documents'][i],metadata=list['metadatas'][i])\n",
    "            literal_docs.append(doc)\n",
    "        \n",
    "        docs_l2 = self.vs.similarity_search(query)\n",
    "\n",
    "        docs_simcos = self.vs.similarity_search_by_vector(hugg_embeddings.embed_query(query))\n",
    "        \n",
    "        docs_simcos_bge = self.vs_2.similarity_search_by_vector(hugg_emb_bgem3.embed_query(query))\n",
    "        #Scores acumulados de cada haciendo uso de cada tecnica\n",
    "        scores_literal = self.fused_scores_literal(literal_docs)\n",
    "\n",
    "        scores_l2 = self.get_fused_scores(scores_literal,docs_l2)\n",
    "        \n",
    "        scores_simcos = self.get_fused_scores(scores_l2,docs_simcos)\n",
    "\n",
    "        scores_bge = self.get_fused_scores(scores_simcos,docs_simcos_bge)\n",
    "        #Recuperación de documentos\n",
    "        rrf_documents = self.rrf(scores_bge)\n",
    "\n",
    "        #evaluación de los documentos con CRAG.\n",
    "        crag_documents = self.CRAG(query,rrf_documents)\n",
    "        \n",
    "        return crag_documents[0:self.k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joseacentic/centic/LLM/envPRUEBA/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "scores previos 0\n",
      "scores previos 0\n",
      "scores previos 0\n",
      "scores previos 0.5\n",
      "scores previos 0.5\n",
      "scores previos 0\n",
      "scores previos 0.5\n",
      "scores previos 0.5\n",
      "scores previos 0\n",
      "scores previos 0\n",
      "scores previos 0.5\n",
      "scores previos 0.5\n",
      "scores previos 1.0\n",
      "scores previos 0.5\n",
      "scores previos 0.8333333333333333\n",
      "scores previos 0.75\n",
      "scores previos 2.0\n",
      "scores previos 1.0\n",
      "scores previos 1.0\n",
      "scores previos 0.5\n",
      "Documentos finales (score)\n",
      "------\n",
      "3.0\n",
      "------\n",
      "1.5\n",
      "------\n",
      "1.3333333333333333\n",
      "------\n",
      "1.1666666666666665\n",
      "------\n",
      "0.75\n",
      "------\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joseacentic/centic/LLM/envPRUEBA/lib/python3.10/site-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\n",
      "\"score\": \"yes\"\n",
      "} {\"score\": \"yes\"} {\n",
      "\"score\": \"no\"\n",
      "} {\"score\": \"yes\"} {\"score\": \"yes\"} {\n",
      "\"score\": false\n",
      "}\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a knowledgeable chatbot, here to help with questions of the user.\n",
      "    Your tone should be professional and informative.\n",
      "\n",
      "    Context: sector and one of the UK’s leading \n",
      "entrepreneurs and philanthropists. Along \n",
      "with his brother Farouq, he co-founded \n",
      "CareTech. As Group CEO he actively leads \n",
      "the day-to-day running of the Group \n",
      "and its international expansion, and has \n",
      "been instrumental in assembling a highly \n",
      "talented leadership team, to support \n",
      "the continued growth of the business. \n",
      "Haroon brings commercial acumen, \n",
      "related industry experience and property \n",
      "knowledge. He has a deep commitment\n",
      "\n",
      "of those in our care. \n",
      "Our vision has driven our success. Our purpose \n",
      "is shared by our staff as well as by the people \n",
      "in our care. We want everyone in the CareTech \n",
      "family to have a bright future and we work \n",
      "tirelessly towards that aim. \n",
      "Haroon Sheikh\n",
      "Group Chief Executive Officer\n",
      "6 December 2021\n",
      "Our Purpose\n",
      "We enable children, young people \n",
      "and adults with complex needs to \n",
      "make their own life choices, and build \n",
      "confidence and independence to live,\n",
      "\n",
      "GOVERNANCEBOARD OF  \n",
      "DIRECTORS\n",
      "APPOINTED\n",
      "SKILLS AND \n",
      "EXPERIENCE\n",
      "Farouq Sheikh OBE\n",
      "Group Executive Chairman\n",
      "Aged 63Haroon Sheikh\n",
      "BSc Group Chief  \n",
      "Executive Officer\n",
      "Aged 65Christopher Dickinson\n",
      "Chief Financial Officer\n",
      "Aged 43Mike Adams OBE\n",
      "Executive Director\n",
      "Aged 50Jamie Cumming\n",
      "Non-Executive Director\n",
      "Aged 71Karl Monaghan\n",
      "Non-Executive Director\n",
      "Aged 59Prof. Moira Livingston\n",
      "Non-Executive Director\n",
      "Aged 59\n",
      "Farouq has been a key architect in \n",
      "CareTech’s growth, having co-founded\n",
      "\n",
      "As we continue to live with the \n",
      "effects of the global pandemic, \n",
      "CareTech has demonstrated the \n",
      "resoluteness of our purpose and \n",
      "resilience in our business model.Group Chief Executive’s Statement and Performance Review\n",
      "Haroon SheikhAs we continue to live with the effects of the \n",
      "global COVID-19 pandemic, CareTech has \n",
      "remained resolute in its purpose to deliver the \n",
      "highest quality of care while demonstrating \n",
      "the resilience of our business model. These \n",
      "two fundamentals are the basis for our\n",
      "    History: []\n",
      "\n",
      "    User: Is it true that Haroon Sheikh is the CEO of CareTech?\n",
      "    Chatbot:\n",
      "    It is mandatorian that only if the answer is not in the context, answer \"I have not enough context in order to answer this\" and stop the answer.\n",
      "    Try to use the memory context in the answer only if the question mentions it.\n",
      "\u001b[0m\n",
      " Yes, Haroon Sheikh is the Group Chief Executive Officer of CareTech as stated in the provided context.\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Is it true that Haroon Sheikh is the CEO of CareTech?',\n",
       " 'result': ' Yes, Haroon Sheikh is the Group Chief Executive Officer of CareTech as stated in the provided context.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "    You are a knowledgeable chatbot, here to help with questions of the user.\n",
    "    Your tone should be professional and informative.\n",
    "\n",
    "    Context: {context}\n",
    "    History: {history}\n",
    "\n",
    "    User: {question}\n",
    "    Chatbot:\n",
    "    It is mandatorian that only if the answer is not in the context, answer \"I have not enough context in order to answer this\" and stop the answer.\n",
    "    Try to use the memory context in the answer only if the question mentions it.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"context\", \"question\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"history\",\n",
    "    return_messages=True,\n",
    "    input_key=\"question\"\n",
    ")\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',\n",
    "    retriever=CustomRetriever_advanced(vs = vectorstore, vs_2=vectorstore_bgem3, k=4),\n",
    "    verbose=True,\n",
    "    chain_type_kwargs={\n",
    "        \"verbose\": True,\n",
    "        \"prompt\": prompt,\n",
    "        \"memory\": memory,\n",
    "    }\n",
    ")\n",
    "query = 'Is it true that Haroon Sheikh is the CEO of CareTech?'\n",
    "result = qa_chain({\"query\": query})\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envPRUEBA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
