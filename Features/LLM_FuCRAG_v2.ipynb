{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Fusion corrective RAG with parent/child strategy</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.vectorstores import VectorStore \n",
    "from langchain_core.documents.base import Document\n",
    "from langchain.load import dumps, loads\n",
    "\n",
    "\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from typing import List\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joseacentic/centic/LLM/envPRUEBA/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(base_url=\"http://localhost:11434\",\n",
    "             model=\"mistral\",\n",
    "             verbose=True,  \n",
    "             callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))\n",
    "\n",
    "hugg_embeddings = HuggingFaceEmbeddings(model_name= \"mxbai-embed-large-v1\")\n",
    "\n",
    "hugg_emb_bgem3 = HuggingFaceEmbeddings(model_name = \"BAAI/bge-m3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "myFile = 'annualreport.pdf'\n",
    "loader = PyPDFLoader(\"DATA/\" + myFile)\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 2000,\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "child_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=400,\n",
    "        length_function=len\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Nota:</u> \n",
    "\n",
    "Ver si es posible unir las dos celdas siguientes en una única base de datos añadiendo una nueva colección de documentos. \n",
    "\n",
    "No está claro por la forma en la que se tiene que acceder a los documentos de la base de datos desde el retriever personalizado. \n",
    "\n",
    "<del>IMPORTANTE: Comprobar que cuando se define el store = InMemoryStore() dos veces (una para cada modelo de embeddings) tiene la misma referencia (doc_id en el metadata) porque si no estaremos recuperando documentos iguales mas de una vez. </del> Tienen referencias distintas\n",
    "\n",
    "<del>OJO puede ser que no haya ningun problema puesto que dentro de la Store se guarde el mismo documento dos veces con dos ids diferentes. (Probar) </del> Esto es lo que ocurre\n",
    "\n",
    "<h4><b>Problema en el reranking al meter el parent child: <u> SOLUCIONADO  </u> </b></h4>  \n",
    "\n",
    "Cuando no se usaba el parent child no habia problema al usar el dumnps(doc) para comprobar los que eran iguales y los que no. Ahora como se generan doc_id diferentes en función de las vectorstore si dos documentos iguales vienen de diferentes bases de datos\n",
    "el dumps() no los asocia como iguales puesto que no tienen el mismo metadata. entonces en el reranking se meten de nuevo a 0 y no suma puntuación si ya está metido en la lista \n",
    "\n",
    "\n",
    "Se arreglaría todo metiendo en la vectorstore nueva los documentos con los mismos doc_id que en la bd anterior. --- > No se ha solucionado así"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma(\n",
    "        collection_name=\"documents\",\n",
    "        embedding_function=hugg_embeddings\n",
    "    )\n",
    "\n",
    "store = InMemoryStore()\n",
    "\n",
    "full_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter\n",
    ")\n",
    "\n",
    "full_retriever.add_documents(data, ids = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_bgem3 = Chroma(\n",
    "        collection_name=\"documents\",\n",
    "        embedding_function=hugg_emb_bgem3\n",
    "    )\n",
    "\n",
    "#store_bge = InMemoryStore()\n",
    "\n",
    "full_retriever_bge = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore_bgem3,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter\n",
    ")\n",
    "\n",
    "full_retriever_bge.add_documents(data, ids = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='number of AIM quoted and private \\ncompany boards.January 2020 October 2005\\nHaroon is amongst the most experienced \\nCEOs in the health and social care \\nsector and one of the UK’s leading \\nentrepreneurs and philanthropists. Along \\nwith his brother Farouq, he co-founded \\nCareTech. As Group CEO he actively leads \\nthe day-to-day running of the Group \\nand its international expansion, and has', metadata={'doc_id': '0fd61114-6759-4a9b-9eb0-b7b5efd8dad4', 'page': 40, 'source': 'DATA/annualreport.pdf'}),\n",
       " Document(page_content='with his brother Farouq, he co-founded \\nCareTech. As Group CEO he actively leads \\nthe day-to-day running of the Group \\nand its international expansion, and has \\nbeen instrumental in assembling a highly \\ntalented leadership team, to support \\nthe continued growth of the business. \\nHaroon brings commercial acumen, \\nrelated industry experience and property \\nknowledge. He has a deep commitment', metadata={'doc_id': '0fd61114-6759-4a9b-9eb0-b7b5efd8dad4', 'page': 40, 'source': 'DATA/annualreport.pdf'}),\n",
       " Document(page_content='is shared by our staff as well as by the people \\nin our care. We want everyone in the CareTech \\nfamily to have a bright future and we work \\ntirelessly towards that aim. \\nHaroon Sheikh\\nGroup Chief Executive Officer\\n6 December 2021\\nOur Purpose\\nWe enable children, young people \\nand adults with complex needs to \\nmake their own life choices, and build \\nconfidence and independence to live,', metadata={'doc_id': 'a69584e9-de60-4200-918f-361e8b051ea0', 'page': 2, 'source': 'DATA/annualreport.pdf'}),\n",
       " Document(page_content='Haroon Sheikh and Christopher Dickinson under the Sharesave Scheme.\\nChristopher Dickinson has an interest in 155,250 Ordinary Shares in CareTech pursuant to the Executive Shared Ownership Plan, details of  \\nwhich were announced on 8 November 2019.\\nNone of the options above are subject to clawback arrangements\\nNo other Director has any share options in the Group.\\nNon-Executive Director Fees', metadata={'doc_id': 'cb08787a-c858-40a4-b4d3-5a1c3517edc5', 'page': 50, 'source': 'DATA/annualreport.pdf'})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore_bgem3.similarity_search('Is it true that Haroon Sheikh is the CEO of CareTech?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='number of AIM quoted and private \\ncompany boards.January 2020 October 2005\\nHaroon is amongst the most experienced \\nCEOs in the health and social care \\nsector and one of the UK’s leading \\nentrepreneurs and philanthropists. Along \\nwith his brother Farouq, he co-founded \\nCareTech. As Group CEO he actively leads \\nthe day-to-day running of the Group \\nand its international expansion, and has', metadata={'doc_id': '6ee008de-9630-4bbd-b07a-2ff6b0a85ea3', 'page': 40, 'source': 'DATA/annualreport.pdf'}),\n",
       " Document(page_content='with his brother Farouq, he co-founded \\nCareTech. As Group CEO he actively leads \\nthe day-to-day running of the Group \\nand its international expansion, and has \\nbeen instrumental in assembling a highly \\ntalented leadership team, to support \\nthe continued growth of the business. \\nHaroon brings commercial acumen, \\nrelated industry experience and property \\nknowledge. He has a deep commitment', metadata={'doc_id': '6ee008de-9630-4bbd-b07a-2ff6b0a85ea3', 'page': 40, 'source': 'DATA/annualreport.pdf'}),\n",
       " Document(page_content=\"Awards. In 2019, Haroon and Farouq were \\nwinners of the 'Asian Business of the Year'.\\nHaroon, a graduate of the University \\nof London, is a Founder Trustee of the \\nCareTech Charitable Foundation formed \\nin 2017, and is Chairman of the Trustees, \\nworking closely with the Foundation’s \\nCEO and independent Trustees.Jamie joined the Board as a Non-\\nExecutive Director in 2013. Following\", metadata={'doc_id': '6ee008de-9630-4bbd-b07a-2ff6b0a85ea3', 'page': 40, 'source': 'DATA/annualreport.pdf'}),\n",
       " Document(page_content='is shared by our staff as well as by the people \\nin our care. We want everyone in the CareTech \\nfamily to have a bright future and we work \\ntirelessly towards that aim. \\nHaroon Sheikh\\nGroup Chief Executive Officer\\n6 December 2021\\nOur Purpose\\nWe enable children, young people \\nand adults with complex needs to \\nmake their own life choices, and build \\nconfidence and independence to live,', metadata={'doc_id': 'f5d6c638-4710-43a9-83b7-cbdd5df5834e', 'page': 2, 'source': 'DATA/annualreport.pdf'})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search('Is it true that Haroon Sheikh is the CEO of CareTech?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Explicación del retriever, en particular la función _get_relevant_documents() para saber como usar el retriever cuando se quieran añadir más \n",
    "condiciones en el rrf: \n",
    "Esta clase está estructurada para que con las funciones rrf() y get_fused_scores() se puedan ir añadiendo tantos metodos de recuperación de \n",
    "contexto como se quiera. (Siempre que los documentos recuperados tengan un orden concreto, para los que no, ya está el método fused_scores_literal()\n",
    "que asigna a todos los documentos el mismo valor). \n",
    "¿Cómo funciona y se puede automatizar la función _get_relevant_documents()?\n",
    "Supongamos que tenemos n criterios de recuperación de contextos (en este ejemplo tenemos 4, literal, mxbai l2, mxbai cos, bge cos)\n",
    "En primer lugar, se necestita ir calculando los scores de los documentos que vamos obteniendo para ello, se va haciendo uso de la función get_fused_scores()\n",
    "scores1 =  get_fused_scores({},lista1), scores2 = get_fused_scores(scores1,lista2), ... , scoresn = get_fused_scores(scores(n-1), listan)\n",
    "Esto lo que va haciendo es ir acumulando los scores de acuerdo al valor que se ha decidido dar. En este caso, 1/1+rank. \n",
    "Por ultimo una vez tengamos todos los documentos con sus scores se hace uso de la función rrf con los ultimos scores, que se encarga de dar la lista final de \n",
    "documentos ordenada sin los scores ya para que la procese la función del retriever. \n",
    "rrf(scores(n)) puesto que esta función desde dentro ya se encarga de calcular los scores para la ultima lista.\n",
    "'''\n",
    "class CustomRetriever_advanced(BaseRetriever):\n",
    "    '''En este caso se necesitan dos db vectoriales que almacenen los embeddings que genera cada modelo diferente. Se pueden usar\n",
    "    tantos modelos de generación de embeddings como se quiera simplemente habrá que pasar en esta clase del retriever personalizado \n",
    "    las bases de datos correspondientes. También se va a añadir una variable k que será el número de documentos que se quieran recuperar\n",
    "    Por ejemplo si k = 10 pues el retriever generará un contexto con los 10 documentos que mayor score hayan generado. En el caso anterior \n",
    "    que k no se especificaba, en los documentos se metían todos los que se iban recuperando de cada método. '''\n",
    "    vs: VectorStore\n",
    "    vs_2: VectorStore\n",
    "    k: int\n",
    "    '''fused_scores es un diccionario que va a ir actualizando la solucion de rrf por lo que en la primera\n",
    "    iteración fused_scores = {} (diccionario vacio) mientras que para las demás iteraciones será la solución\n",
    "    de esta función junto con las diferentes listas que queramos ir pasandole el rrf'''\n",
    "    \n",
    "    '''\n",
    "    Cada vez que se hace una llamada a la función rrf nos da una lista de objetos Document (sin el score) que están ya listos \n",
    "    para pasar al retrievalQA. Es necesario pasarle la lista de scores que se va obteniendo conforme se van usando técnicas en\n",
    "    el reranking. \n",
    "    '''\n",
    "    def rrf(self,fused_scores):\n",
    "        reranked_results = {doc: score for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)}\n",
    "        #falta pasar estos resultados a una lista de documentos que es lo que devuelve el retriver (loads)\n",
    "        lista_rerank = []\n",
    "        print('Documentos finales (score)')\n",
    "        for doc, score in reranked_results.items():\n",
    "            lista_rerank.append(loads(doc))\n",
    "            print('------')\n",
    "            print(score)\n",
    "        return lista_rerank\n",
    "    \n",
    "    '''\n",
    "    La función get_fused_scores, nos sirve para ir actualizando los scores de una lista de documetnos dada. Por ejemplo, si estamos \n",
    "    en la primera iteración del rrf, previus_fused = {} y esta función devolverá un diccionario actualizado con los documetnos de la lista \n",
    "    y su correspondiente score. Este diccionario será necesario para pasarselo luego en la función rrf. \n",
    "    Cuando no estamos en la primera iteración y previus_fused ya no es un diccionario vacio, se le pasará un diccionario de documentos y \n",
    "    scores junto con una nueva lista que tendrá los documentos que queremos incorporar a la lista de scores. \n",
    "    '''   \n",
    "    def get_fused_scores(self,previus_fused, lista):\n",
    "        for rank, doc in enumerate(lista):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in previus_fused:\n",
    "                previus_fused[doc_str] = 0\n",
    "            previus_score = previus_fused[doc_str]\n",
    "            print(f'scores previos {previus_score}')\n",
    "            previus_fused[doc_str] += 1 / (rank + 1)\n",
    "        return previus_fused\n",
    "    \n",
    "    '''\n",
    "    Este método realiza la misma función que la anterior solo que en esta se ha utilizado para arreglar el problema de tener doc_id diferente para el\n",
    "    mismo chunk en dos bases de datos. Lo que hace es ir metiendo los documentos en una lista y actualizar el metadata de los nuevos que van entrando si \n",
    "    ya se encuentran en la lista. Asi solucionamos el tema de la duplicidad, el resto de funcionamiento es idéntico.\n",
    "    '''\n",
    "    def get_fused_scores_v2(self,previus_fused, lista):\n",
    "        \n",
    "        previus_list = []\n",
    "        for doc, score in previus_fused.items():\n",
    "            previus_list.append(loads(doc))\n",
    "        \n",
    "        for d in lista:\n",
    "            for p_d in previus_list:\n",
    "                if d.page_content == p_d.page_content: \n",
    "                    d.metadata['doc_id'] = p_d.metadata['doc_id']\n",
    "\n",
    "        for rank, doc in enumerate(lista):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in previus_fused:\n",
    "                previus_fused[doc_str] = 0\n",
    "            previus_score = previus_fused[doc_str]\n",
    "            print(f'scores previos {previus_score}')\n",
    "            previus_fused[doc_str] += 1 / (rank + 1)\n",
    "\n",
    "        return previus_fused\n",
    "\n",
    "\n",
    "    '''\n",
    "    Nota: Esta función no haría falta si no se le diera la misma puntuación a cada documento obtenido en la búsqueda literal. Esto se debe a que \n",
    "    cuando se buscan documentos de forma literal todos han de tener el mismo valor. No se puede distinguir de esta manera si hay uno \"mejor\" que otro. \n",
    "    Como aqui tenemos fused_scores = {}, esto significa que si queremos usar esta estrategia siempre tiene que ser los primeros scores que se recopilan, \n",
    "    si no hay que cambiar y pasarle como argumetno unos scores previos y acumular con += 0.5\n",
    "    En resumen no es necesario aqui comprobar lo de la lista auxiliar y cambiar el metadata de doc_id ya que en este caso serían los primeros documentos \n",
    "    que entrarían en la lista y en ningún caso estarían duplicados. \n",
    "    '''\n",
    "    def fused_scores_literal(self, lista):\n",
    "        fused_scores = {}\n",
    "        for doc in lista:\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            previus_score = fused_scores[doc_str]\n",
    "            print(f'scores previos {previus_score}')\n",
    "            fused_scores[doc_str] = 0.5\n",
    "        return fused_scores\n",
    "    ''' \n",
    "    Genera una consulta para que sea dinámica la busqueda literal en la base de datos, dada una lista de palabras clave. \n",
    "    El método de busqueda literal en la base de datos chroma, necesita el siguiente formato\n",
    "    {\n",
    "    \"$and\": [\n",
    "        {\"$contains\": \"key_word1\"},\n",
    "        {\"$contains\": \"key_word2\"}\n",
    "        ]\n",
    "    }\n",
    "    Con lo cual, este método en función de las palabras clave que tenga nuestra query, se van añadiendo a la consulta y no se\n",
    "    necesita ir creando una consulta a mano para cada búsqueda literal que tenga diferente numero de palabras clave. \n",
    "    '''\n",
    "    def generaConsulta(self,key_words):\n",
    "        busquedas = []\n",
    "        for w in key_words:\n",
    "            conta = {\"$contains\":str(w)}\n",
    "            busquedas.append(conta)\n",
    "        #Duda: Poner un or en vez de un and. En el caso de que existan muchos keywords puede ser dificil encontrar documentos que los contengan todas. \n",
    "        consulta = {\"$and\": busquedas}\n",
    "        return consulta\n",
    "    \n",
    "    '''Corrective RAG:\n",
    "    La función CRAG, recibe una query (pregunta) y una serie de documentos que han sido recuperados con el objetivo de responder la pregunta\n",
    "    de forma correcta. En este caso, se crea un LLM que sea capaz de evaluar cada uno de estos documentos y decida si su contenido es adecuado\n",
    "    para responder la pregunta. Los documentos que se consideren adecuados son aquellos que se devuelven en esta función. \n",
    "    '''\n",
    "    def CRAG(self, query, docs):\n",
    "\n",
    "        prompt = PromptTemplate(\n",
    "            template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "            Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "            Here is the user question: {question} \\n\n",
    "            If the document contains keywords related to the user question, grade it as relevant. \\n\n",
    "            It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "            Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "            Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\"\"\",\n",
    "            input_variables=[\"question\", \"document\"],\n",
    "        )\n",
    "\n",
    "        retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "\n",
    "        valid_docs = []\n",
    "\n",
    "        for d in docs:\n",
    "            score = retrieval_grader.invoke({\"question\": query, \"document\": d.page_content})\n",
    "            if score['score'] == \"yes\":\n",
    "                valid_docs.append(d)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        return valid_docs\n",
    "    \n",
    "    '''\n",
    "    En el RetrievalQA, que se pasa en la cadena, el retriever que se usa por la clase para obtener los diferentes documentos hace uso de una función\n",
    "    llamada get_relevant_documents() que viene predeterminada en el retriever base. Cuando hacemos un retriever personalizado, es necesario crear una\n",
    "    función _get_relevant_documents(), con _ delante del mismo nombre anterior para que a la hora de recuperar documentos la función RetrievalQA haga \n",
    "    uso de la función que hemos personalizado para recuperar los documentos. Solo tenemos que tener en cuenta que al aplicar los cambios que queremos\n",
    "    al personalizar esta función, se devuelva una lista de documentos (objetos Document)\n",
    "    '''\n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
    "        ) -> List[Document]:\n",
    "        #documentos\n",
    "        spacyModel = spacy.load(\"en_core_web_sm\")\n",
    "        list = self.vs.get(\n",
    "            where_document=self.generaConsulta(spacyModel(query).ents)\n",
    "            )\n",
    "        \n",
    "        literal_docs = []\n",
    "        for i in range(len(list['ids'])):\n",
    "            doc = Document(page_content=list['documents'][i],metadata=list['metadatas'][i])\n",
    "            literal_docs.append(doc)\n",
    "        \n",
    "        docs_l2 = self.vs.similarity_search(query)\n",
    "\n",
    "        docs_simcos = self.vs.similarity_search_by_vector(hugg_embeddings.embed_query(query))\n",
    "        \n",
    "        docs_simcos_bge = self.vs_2.similarity_search_by_vector(hugg_emb_bgem3.embed_query(query))\n",
    "        #Scores acumulados de cada haciendo uso de cada tecnica\n",
    "        scores_literal = self.fused_scores_literal(literal_docs)\n",
    "\n",
    "        scores_l2 = self.get_fused_scores_v2(scores_literal,docs_l2)\n",
    "        \n",
    "        scores_simcos = self.get_fused_scores_v2(scores_l2,docs_simcos)\n",
    "\n",
    "        scores_bge = self.get_fused_scores_v2(scores_simcos,docs_simcos_bge)\n",
    "        #Recuperación de documentos\n",
    "        rrf_documents = self.rrf(scores_bge)\n",
    "\n",
    "        #evaluación de los documentos con CRAG.\n",
    "        crag_documents = self.CRAG(query,rrf_documents)\n",
    "\n",
    "        ids = []\n",
    "\n",
    "        for d in crag_documents: \n",
    "            if d.metadata['doc_id'] not in ids: \n",
    "                ids.append(d.metadata['doc_id'])\n",
    "\n",
    "        parent_docs = store.mget(ids)\n",
    "\n",
    "        str_docs = []\n",
    "        docs = []\n",
    "        for pd in parent_docs:\n",
    "            parent_doc_str = dumps(pd)\n",
    "            if parent_doc_str not in str_docs:\n",
    "                if pd is not None:\n",
    "                    str_docs.append(parent_doc_str)\n",
    "                    docs.append(loads(parent_doc_str))\n",
    "\n",
    "        return docs[0:self.k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joseacentic/centic/LLM/envPRUEBA/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "scores previos 0\n",
      "scores previos 0\n",
      "scores previos 0\n",
      "scores previos 0\n",
      "scores previos 0\n",
      "scores previos 0\n",
      "scores previos 0\n",
      "scores previos 0\n",
      "scores previos 0\n",
      "scores previos 0\n",
      "scores previos 0\n",
      "scores previos 0\n",
      "scores previos 0\n",
      "scores previos 0\n",
      "scores previos 0\n",
      "scores previos 0\n",
      "scores previos 0\n",
      "scores previos 0.5\n",
      "scores previos 1.0\n",
      "scores previos 0.5\n",
      "scores previos 0.3333333333333333\n",
      "scores previos 0.75\n",
      "scores previos 2.0\n",
      "scores previos 1.0\n",
      "scores previos 1.0\n",
      "scores previos 0.5\n",
      "Documentos finales (score)\n",
      "------\n",
      "3.0\n",
      "------\n",
      "1.5\n",
      "------\n",
      "1.3333333333333333\n",
      "------\n",
      "0.75\n",
      "------\n",
      "0.6666666666666666\n",
      "------\n",
      "0.5\n",
      "------\n",
      "0.5\n",
      "------\n",
      "0.5\n",
      "------\n",
      "0.5\n",
      "------\n",
      "0.5\n",
      "------\n",
      "0.5\n",
      "------\n",
      "0.5\n",
      "------\n",
      "0.5\n",
      "------\n",
      "0.5\n",
      "------\n",
      "0.5\n",
      "------\n",
      "0.5\n",
      "------\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joseacentic/centic/LLM/envPRUEBA/lib/python3.10/site-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\"score\": \"yes\"} {\"score\": \"yes\"} {\"score\": \"yes\"} {\n",
      "\"score\": \"no\"\n",
      "} {\n",
      "\"score\": \"no\"\n",
      "} {\"score\": \"yes\"} {\n",
      "\"score\": \"no\"\n",
      "} {\n",
      "\"score\": \"no\"\n",
      "} {\n",
      "\"score\": \"yes\"\n",
      "} {\"score\": \"yes\"} {\n",
      "\"score\": \"no\"\n",
      "} {\"score\": \"yes\"} {\n",
      "\"score\": \"yes\"\n",
      "} {\n",
      "\"score\": \"no\"\n",
      "} {\n",
      "\"score\": \"no\"\n",
      "} {\n",
      "\"score\": \"yes\"\n",
      "} {\n",
      "\"score\": \"yes\"\n",
      "}\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a knowledgeable chatbot, here to help with questions of the user.\n",
      "    Your tone should be professional and informative.\n",
      "\n",
      "    Context: and Robert W. Baird for two years until \n",
      "June 2002. Karl set up Ashling Capital \n",
      "LLP in December 2002 to provide \n",
      "consultancy services to quoted \n",
      "and private companies. He sits on a \n",
      "number of AIM quoted and private \n",
      "company boards.January 2020 October 2005\n",
      "Haroon is amongst the most experienced \n",
      "CEOs in the health and social care \n",
      "sector and one of the UK’s leading \n",
      "entrepreneurs and philanthropists. Along \n",
      "with his brother Farouq, he co-founded \n",
      "CareTech. As Group CEO he actively leads \n",
      "the day-to-day running of the Group \n",
      "and its international expansion, and has \n",
      "been instrumental in assembling a highly \n",
      "talented leadership team, to support \n",
      "the continued growth of the business. \n",
      "Haroon brings commercial acumen, \n",
      "related industry experience and property \n",
      "knowledge. He has a deep commitment \n",
      "and passion for delivering high-quality \n",
      "care and support to people with  \n",
      "complex needs. \n",
      "Haroon is Patron and Enterprise Fellow of \n",
      "the Prince’s Trust and is a member of the \n",
      "UK Advisory Council of the British Asian \n",
      "Trust under the patronage of HRH The \n",
      "Prince of Wales.\n",
      "In 2008, Haroon and Farouq were \n",
      "winners of the highly valued Coutts \n",
      "Family Business Prize and widely \n",
      "applauded for the quality and social \n",
      "integrity of the business they created. \n",
      "In 2009, they were both finalists in the \n",
      "Ernst & Young Entrepreneur of the Year \n",
      "Awards and in 2016 they received the \n",
      "Outstanding Contribution Award at \n",
      "the Laing & Buisson Annual Healthcare \n",
      "Awards. In 2019, Haroon and Farouq were \n",
      "winners of the 'Asian Business of the Year'.\n",
      "Haroon, a graduate of the University \n",
      "of London, is a Founder Trustee of the \n",
      "CareTech Charitable Foundation formed \n",
      "in 2017, and is Chairman of the Trustees, \n",
      "working closely with the Foundation’s \n",
      "CEO and independent Trustees.Jamie joined the Board as a Non-\n",
      "Executive Director in 2013. Following \n",
      "a long career in corporate advisory \n",
      "and broking in the City, including \n",
      "acting as Chief Executive Officer of \n",
      "N+1Brewin LLP, and latterly as Senior\n",
      "\n",
      "StRAtEgIC REP oRt\n",
      "Independence means having the confidence \n",
      "and skills to be able to live, learn, thrive and \n",
      "engage in community activities – a future \n",
      "that is unique to them and informed by \n",
      "their choices. \n",
      "We help them to master the essentials of daily \n",
      "life, like being able to cook, shop, keep house, \n",
      "build relationships and stay well and safe.  \n",
      "We provide access to learning – academic or \n",
      "playful. We build the confidence and skills for \n",
      "employment for those who would like to work \n",
      "in any capacity. And we nurture the ability and \n",
      "self-awareness to engage socially and build \n",
      "lasting relationships. We achieve all of this thanks to the loving, \n",
      "supportive, fun and therapeutic environments \n",
      "that our specialist professional teams create \n",
      "and with the courage and determination  \n",
      "of those in our care. \n",
      "Our vision has driven our success. Our purpose \n",
      "is shared by our staff as well as by the people \n",
      "in our care. We want everyone in the CareTech \n",
      "family to have a bright future and we work \n",
      "tirelessly towards that aim. \n",
      "Haroon Sheikh\n",
      "Group Chief Executive Officer\n",
      "6 December 2021\n",
      "Our Purpose\n",
      "We enable children, young people \n",
      "and adults with complex needs to \n",
      "make their own life choices, and build \n",
      "confidence and independence to live, \n",
      "learn, thrive and engage  in their families \n",
      "and communities for more independent \n",
      "futures that meet their aspirations.\n",
      "OUR PURPOSE\n",
      "OUR VISION IS FOR A WORLD WHERE \n",
      "THERE IS EQUAL OPPORTUNITY FOR \n",
      "A LIFE FULLY LIVED.\n",
      "03StRAtEgIC REPoRt\n",
      "02\n",
      "CareTech Holdings PLC / Annual Report and Accounts 2021\n",
      "\n",
      "the sale or other disposal of the shares. Malus \n",
      "and clawback provisions are also in place to \n",
      "reduce or recover the awards in circumstances \n",
      "such as any material misstatement of the \n",
      "financial statements, or a serious breach  \n",
      "of the Company's code of ethics.The first LTIP award was granted to participants in December 2020. The LTIP awards were set \n",
      "at 150% of salary for the Group Executive Chairman, Chief Executive Officer and Group Chief \n",
      "Financial Officer and 75% for the Executive Director and subject to EPS growth over the  \n",
      "three- year performance period. Further details are set out later on in this report. \n",
      "Remuneration decisions for 2022 \n",
      "Salaries\n",
      "The following table sets out salaries for Executive Directors for the year ended 30 September \n",
      "2021 and intended salaries effective for the year ending 30 September 2022. \n",
      "Executive DirectorsSalary with  \n",
      "effect from  \n",
      "1 October 2020Salary with  \n",
      "effect from  \n",
      "1 October 2021\n",
      "Group Executive Chairman Farouq Sheikh OBE £400,000 £400,000 (+0%)\n",
      "Group Chief Executive Officer Haroon Sheikh £450,000 £450,000 (+0%) \n",
      "Group Chief Financial Officer Christopher Dickinson £300,000 £300,000 (+0%)\n",
      "Executive Director Mike Adams OBE £125,000 £125,000 (+0%) \n",
      "Benefits\n",
      "There will be no changes to benefits provided \n",
      "to the Executive Directors.\n",
      "Relocation allowance\n",
      "In order to grow and develop CareTech in \n",
      "the Gulf region, the Group Chief Executive \n",
      "Officer has relocated on a temporary basis \n",
      "to the United Arab Emirates. The Group \n",
      "Chief Executive Officer has retained all of his \n",
      "current responsibilities, with the international \n",
      "development of the business representing an \n",
      "expansion of his duties and responsibilities.  \n",
      "A relocation allowance of £12,000 per month \n",
      "has been approved by the Remuneration \n",
      "Committee to cover his move and included \n",
      "visa and immigration support, flights, housing \n",
      "and education fees.\n",
      "Annual bonus\n",
      "The maximum bonus opportunity remains  \n",
      "at 100% of salary for the Group Executive\n",
      "\n",
      "As we continue to live with the \n",
      "effects of the global pandemic, \n",
      "CareTech has demonstrated the \n",
      "resoluteness of our purpose and \n",
      "resilience in our business model.Group Chief Executive’s Statement and Performance Review\n",
      "Haroon SheikhAs we continue to live with the effects of the \n",
      "global COVID-19 pandemic, CareTech has \n",
      "remained resolute in its purpose to deliver the \n",
      "highest quality of care while demonstrating \n",
      "the resilience of our business model. These \n",
      "two fundamentals are the basis for our \n",
      "success in delivering Extraordinary Days, \n",
      "Every Day  to transform outcomes for the \n",
      "people in our care and provide value for our \n",
      "commissioners.\n",
      "our commitment to high-quality care\n",
      "Delivering the highest standards in education, \n",
      "support and care, and striving to continually \n",
      "improve outcomes for children, young people \n",
      "and adults, are the cornerstones of our \n",
      "purpose as a Group. Our recipe for success, \n",
      "tried and tested over two decades, is an \n",
      "unrelenting focus on quality, which in turn \n",
      "drives commercial success. \n",
      "These principles have remained particularly \n",
      "important during the pandemic which has \n",
      "caused so much disruption to society. During \n",
      "this time, we have remained focused on \n",
      "providing certainty and assurance to the \n",
      "people in our care that they are our absolute \n",
      "priority, as well as ensuring that our staff feel \n",
      "safe and supported. As the pandemic has eased, both CQC and \n",
      "Ofsted have re-commenced inspections. \n",
      "CareTech’s Adult CQC registered service \n",
      "quality ratings at 30 September 2021 were \n",
      "86% Good or Outstanding (2020: 91%) and \n",
      "our Ofsted ratings at 30 September 2021 \n",
      "were 80% (2020: 82%). Whilst both our CQC \n",
      "and Ofsted ratings compares favourably to \n",
      "the national social care average, we remain \n",
      "committed to providing the highest quality \n",
      "of care across all our services and have \n",
      "comprehensive improvement plans in place \n",
      "to increase our quality ratings further.\n",
      "Throughout the year our Executive-led \n",
      "COVID-19 taskforce has monitored sites\n",
      "    History: []\n",
      "\n",
      "    User: Is it true that Haroon Sheikh is the CEO of CareTech?\n",
      "    Chatbot:\n",
      "    It is mandatorian that only if the answer is not in the context, answer \"I have not enough context in order to answer this\" and stop the answer.\n",
      "    Try to use the memory context in the answer only if the question mentions it.\n",
      "\u001b[0m\n",
      " Yes, Haroon Sheikh is identified as the Group Chief Executive Officer of CareTech Holdings PLC in the text provided.\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Is it true that Haroon Sheikh is the CEO of CareTech?',\n",
       " 'result': ' Yes, Haroon Sheikh is identified as the Group Chief Executive Officer of CareTech Holdings PLC in the text provided.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "    You are a knowledgeable chatbot, here to help with questions of the user.\n",
    "    Your tone should be professional and informative.\n",
    "\n",
    "    Context: {context}\n",
    "    History: {history}\n",
    "\n",
    "    User: {question}\n",
    "    Chatbot:\n",
    "    It is mandatorian that only if the answer is not in the context, answer \"I have not enough context in order to answer this\" and stop the answer.\n",
    "    Try to use the memory context in the answer only if the question mentions it.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"context\", \"question\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"history\",\n",
    "    return_messages=True,\n",
    "    input_key=\"question\"\n",
    ")\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',\n",
    "    retriever=CustomRetriever_advanced(vs = vectorstore, vs_2=vectorstore_bgem3, k=4),\n",
    "    verbose=True,\n",
    "    chain_type_kwargs={\n",
    "        \"verbose\": True,\n",
    "        \"prompt\": prompt,\n",
    "        \"memory\": memory,\n",
    "    }\n",
    ")\n",
    "query = 'Is it true that Haroon Sheikh is the CEO of CareTech?'\n",
    "result = qa_chain({\"query\": query})\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envPRUEBA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
